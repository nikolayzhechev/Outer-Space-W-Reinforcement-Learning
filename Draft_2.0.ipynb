{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01caca51-8e5e-49c2-b2c0-b0ec9d6a7f51",
   "metadata": {},
   "source": [
    "In Draft, tested were a few approaches, however Basilisk and Basilisk reinforecement learning environment were not stable, prone to errors and were difficult to configure. Continuing with a second approach: implementing a custom spacecraft reincforecemtn learning environemnt.\n",
    "\n",
    "Using Gymnasium docs: https://gymnasium.farama.org/introduction/create_custom_env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa51150a-cded-4422-9ce7-20bd611295eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725be2a2-9308-4086-8c11-a4a7fd0e06d9",
   "metadata": {},
   "source": [
    "#### Custom Gym Environment Arhcitecture\n",
    "This section discusses the environment as a whole and identifies overall considerations, goals and design.\n",
    "\n",
    "- compute environment state into observation;\n",
    "- `reset()` function to initiate a new episode for the environment;\n",
    "- `step()` to compute the new state of the environment;\n",
    "\n",
    "\n",
    "When will a reward be given? \n",
    "- spacecraft's target destination;\n",
    "- efficiency or precision;\n",
    "\n",
    "When will termination occure?\n",
    "- goal reached;\n",
    "- loss of fuel;\n",
    "- collision with another object;\n",
    "- error in path resulting to the final destination being impossible to reach;\n",
    "\n",
    "Agent Goals:\n",
    "- Landing: Soft-landing on a planetary surface;\n",
    "- Interplanetary Transfer: Navigating from one celestial body to another;\n",
    "- Orbit Change: Transitioning between different orbits efficiently;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee705b9d-5a4e-4c3e-8c3e-de0313bee52f",
   "metadata": {},
   "source": [
    "**Observation space:**\n",
    "- perceiving the environment;\n",
    "- include enough details and information for the agent;\n",
    "- consider: framce of reference, coordinate system (Cartesian, Keplerian), sensor errors (noise);\n",
    "\n",
    "State variables:\n",
    "\n",
    "| Variable      | Description   |\n",
    "| ------------- | ------------- |\n",
    "| $ x, y, z $ | Position in space (Cartesian) or orbital elements |\n",
    "| $ v_x, v_y, v_z $ | Velocity vector |\n",
    "| $ \\theta, \\phi, \\psi $ | Orientation (Euler angles) |\n",
    "| $ w_x, w_y, w_z $ | Angular velocity |\n",
    "| $ f $ | Remaining fuel |\n",
    "| $ d_{target} $ | Distance to target |\n",
    "\n",
    "**Action space:**\n",
    "- thruster commands;\n",
    "- consider: thruster limitation, fuel efficiency (more thrust = more fuel consumption), control delays;\n",
    "\n",
    "Action types:\n",
    "\n",
    "| Action Type      | Example   |\n",
    "| ------------- | ------------- |\n",
    "| Continuous thrust | [Thrust in X, Y, Z] |\n",
    "| Discrete thrusters | ON/OFF for RCS jets |\n",
    "| Rotational control | \t[Torque in Roll, Pitch, Yaw]|\n",
    "\n",
    "\n",
    "**Physics:**  \n",
    "Physics considerations and implementation plus formulas and general physics laws.  \n",
    "\n",
    "- Newtonian Dynamics:  \n",
    "For simpler free space and no gravity motion.  \n",
    "$ F = ma $ to update velocity and position (Newton's Second Law of Motion states that when a force acts on an object, it will cause the object to accelerate. The larger the object's mass, the greater the force will need to be to cause it to accelerate. This Law may be written as force = mass x acceleration).\n",
    "\n",
    "\n",
    "- Otbital Mechanics:  \n",
    "Consider orbital mechanics and forces from celestial bodies.  \n",
    "Every object in the universe attracts every other body in the universe with a force directed along the line of centers of the two objects\n",
    "that is proportional to the product of their masses and inversely proportional to the square of the distance between them,\n",
    "$$ F=\\frac{G_{m_1 m2}}{d^2} $$\n",
    "In this relation, $G$ is the Newton gravitational constant, $m_1$ and $m_2$ are the\n",
    "masses of the primary and secondary bodies, and $d$ is the distance between them.  \n",
    "Kepler’s Equation: it relates the Mean Anomaly ($𝑀$) to the Eccentric Anomaly ($𝐸$) for an orbit with eccentricity $𝑒$:\n",
    "$$ M = E - e \\sin E $$\n",
    "where:\n",
    "    - $M$ = Mean Anomaly ($M = n(t - t_0)$), where $n = \\frac{2_\\pi}{T}$ is the mean motion, $T$ is the orbital period, and $t_0$ is a reference epoch.\n",
    "    - E = Eccentric Anomaly.\n",
    "    - e = Orbital eccentricity.  \n",
    "    Once $E$ is found, the True Anomaly $ν$ (the angle between the position vector and the periapsis) can be determined using:\n",
    "$$ \\tan \\frac{v}{2} = \\sqrt{\\frac{1+e}{1-e}} \\tan \\frac{E}{2}$$\n",
    "The position in the orbital plane is given by:\n",
    "$$ r = \\frac{a(1 - e^2))}{1 + e \\cos v}$$\n",
    "where:\n",
    "    - $r$ = radial distance from the focus (central body).  \n",
    "    - $a$ = semi-major axis.  \n",
    "  The Cartesian coordinates in the orbital plane (before transformation to an inertial frame) are:\n",
    "$$ x = r \\cos v$$ $$ y = r \\sin v $$\n",
    "Implement with: https://www.astropy.org/ and https://docs.poliastro.space/en/stable/index.\n",
    "\n",
    "\n",
    "- Attitude Control:  \n",
    "Rotational dynamics for docking, planetery entry, or pointing. Usega of *quaternions* for rotation.\n",
    "Euler's theorem: The Orientation of a body is uniquely specified by a vector giving the direction of a body axis and a scalar specifying a rotation angle about the axis. \n",
    "\n",
    "- Fuel Consumption Model:  \n",
    "Thrust burns should reduce remaining fuel. Thruster inefficiencies should be considered. Usage of Tsiolkovsky’s rocket equation:\n",
    "$$ \\varDelta v = v_e ln (\\frac{m_i0}{m_f}) $$\n",
    "where:\n",
    "    - $\\varDelta v$ is the change in velocity (delta-v);\n",
    "    - $ v_e $ is the exhaust velocity;\n",
    "    - $ m_0 $ is the initial mass of the rocket;\n",
    "    - $ m_f $ is the final mass of the rocket after the propellant is expended;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb03804-5b16-4d6e-8251-43bfa8eb0896",
   "metadata": {},
   "source": [
    "#### Reward Function\n",
    "Reward function should be shaped well and not only the end goal. The primary goal of the agent should be to reach the target, minimize fuel consumption, maintain stable motion, avoid oscillations, collisions and unsafe conditions. The reward function should balance all these goals.\n",
    "\n",
    "| Reward Type      | Description  |\n",
    "| ------------- | ------------- |\n",
    "| Goal proximity | Reward for getting closer to the target |\n",
    "| Fuel efficiency | Penalize excessive fuel use |\n",
    "| Soft landing | Bonus for smooth landings (low velocity) |\n",
    "| Docking precision | Bonus for aligning position & velocity |\n",
    "| Stability | Penalize excessive rotations |\n",
    "\n",
    "*Phase 1*: Reward based on moving towards the target.  \n",
    "*Phase 2*: Reward for slowing down near the target.  \n",
    "*Phase 3*: Reward only for successful docking.  \n",
    "\n",
    "A combiation of dense reward and spacrse reward can be applyed:\n",
    "- Dense rewards can speed up learning and will provide continuous feedback.\n",
    "- Sparse rewards will the given on task completion, but can be more challening for this scenario.\n",
    "\n",
    "**Reward function design:**\n",
    "1. Proximity: a proximity reward is provided to ensure that the spacecraft moves towards the goal. A **composite reward function** is used to combine multiple rewards.  \n",
    "*Composite rewards combine multiple reward signals into a single function. This is useful in complex tasks where multiple objectives need to be balanced.*  \n",
    "The distance based-reward will be calculated with the negative Euclidean distance:\n",
    "$$ r_{distance} = - \\lVert x - x_{target} \\rVert $$\n",
    "where $x$ is the current position and $x_{target}$ is the target position.\n",
    "> ```python\n",
    "> reward_distance = -np.linalg.norm(self.state[:3] - self.target_position)\n",
    "\n",
    "2. Efficiency: minimizes excessive thrust burns and can penalize thurs usage.  \n",
    "*Fuel penalty* where $a$ is the applied thrust vector:\n",
    "$$ r_{fuel} = - \\lVert a \\rVert $$\n",
    "> ```python\n",
    "> reward_fuel = -np.linalg.norm(action) * 0.01\n",
    "> \n",
    "For fuel *conservation*:  \n",
    "$$ r_{fuel} = \\frac{f_{remaining}}{f_{max}} $$\n",
    "> ```python\n",
    "> reward_fuel = self.state[-1] / self.max_fuel\n",
    ">\n",
    "3. Velocity: the agent needs to match the correct velocity and not overshoot. The velocity for precise docking has to be correct not to overshoot.\n",
    "$$ r_{velocity} = - \\lVert v - v{target} \\rVert $$\n",
    "> ```python\n",
    "> reward_velocity = -np.linalg.norm(self.state[3:6] - self.target_velocity)\n",
    ">\n",
    "4. Orientation: in order to dock a spacecraft needs to align correctly and penalizing misalignment:\n",
    "$$ r_{orientation} = -angle(q, q_{target}) $$\n",
    "where $q$ is the quaternion representing orientation.\n",
    "5. Landing: soft landing and encouraging stability. High velocity should be controlled during ladning anv penalized if the speed is to high:\n",
    "$$ r_{ladning} = -v^2_z $$\n",
    "6. Terminal rewards: in order to achiave correct behaviour, usage of large terminal rewards will be applyed for successfull missions. There will be a large penalty for failiures (for example crashes). Avoiding agent guessing and maximizing learning efficiency.\n",
    "\n",
    "A weighted sum of all the components of the reward function:\n",
    "$$ R = w1r_{distance} + w2r_{fuel} + w3r_{velocity} + w4r_{orientation} + w5r_{landing} + r_{goal} + r_{crash} $$\n",
    "\n",
    "```python\n",
    "reward = (\n",
    "    -np.linalg.norm(self.state[:3] - self.target_position) * 1.0  # Distance penalty\n",
    "    -np.linalg.norm(action) * 0.01  # Fuel efficiency penalty\n",
    "    -np.linalg.norm(self.state[3:6] - self.target_velocity) * 0.5  # Velocity matching\n",
    "    + orientation_penalty(self.state[6:10], self.target_orientation) * 0.3  # Orientation alignment\n",
    "    + (100 if docking_success else 0)  # Goal reward\n",
    "    + (-100 if collision_detected else 0)  # Collision penalty\n",
    ")\n",
    "```\n",
    "Avoid and consider:\n",
    "- avoid unintended ways to for the agent to maximize rewards;\n",
    "- sparce rewards can lead to less exploration of useful actions;\n",
    "- avoid over-penalizing fuel consumption since the agent might not move;\n",
    "- scalling reward terms in similar magnitude to achieve equall learning;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b456-63b2-4832-a27a-591c8af745ca",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Inherit base `Gym` environment to define custom environment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0f062-088e-475b-b7a3-d2be62e3f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9da8c5-ab53-46ab-92df-de5c6ecfc1b4",
   "metadata": {},
   "source": [
    "#### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28cc36-acbe-4cb8-a55b-de3a30b11464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (global)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
