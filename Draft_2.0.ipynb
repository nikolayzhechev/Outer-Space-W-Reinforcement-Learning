{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01caca51-8e5e-49c2-b2c0-b0ec9d6a7f51",
   "metadata": {},
   "source": [
    "In Draft, tested were a few approaches, however Basilisk and Basilisk reinforecement learning environment were not stable, prone to errors and were difficult to configure. Continuing with a second approach: implementing a custom spacecraft reincforecemtn learning environemnt.\n",
    "\n",
    "Using Gymnasium docs: https://gymnasium.farama.org/introduction/create_custom_env/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725be2a2-9308-4086-8c11-a4a7fd0e06d9",
   "metadata": {},
   "source": [
    "#### Custom Gym Environment Arhcitecture\n",
    "This section discusses the environment as a whole and identifies overall considerations, goals and design.\n",
    "\n",
    "- compute environment state into observation;\n",
    "- `reset()` function to initiate a new episode for the environment;\n",
    "- `step()` to compute the new state of the environment;\n",
    "\n",
    "\n",
    "When will a reward be given? \n",
    "- spacecraft's target destination;\n",
    "- efficiency or precision;\n",
    "\n",
    "When will termination occure?\n",
    "- goal reached;\n",
    "- loss of fuel;\n",
    "- collision with another object;\n",
    "- error in path resulting to the final destination being impossible to reach;\n",
    "\n",
    "Agent Goals:\n",
    "- Landing: Soft-landing on a planetary surface;\n",
    "- Interplanetary Transfer: Navigating from one celestial body to another;\n",
    "- Orbit Change: Transitioning between different orbits efficiently;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee705b9d-5a4e-4c3e-8c3e-de0313bee52f",
   "metadata": {},
   "source": [
    "**Observation space:**\n",
    "- perceiving the environment;\n",
    "- include enough details and information for the agent;\n",
    "- consider: framce of reference, coordinate system (Cartesian, Keplerian), sensor errors (noise);\n",
    "\n",
    "State variables:\n",
    "\n",
    "| Variable      | Description   |\n",
    "| ------------- | ------------- |\n",
    "| $ x, y, z $ | Position in space (Cartesian) or orbital elements |\n",
    "| $ v_x, v_y, v_z $ | Velocity vector |\n",
    "| $ \\theta, \\phi, \\psi $ | Orientation (Euler angles) |\n",
    "| $ w_x, w_y, w_z $ | Angular velocity |\n",
    "| $ f $ | Remaining fuel |\n",
    "| $ d_{target} $ | Distance to target |\n",
    "\n",
    "**Action space:**\n",
    "- thruster commands;\n",
    "- consider: thruster limitation, fuel efficiency (more thrust = more fuel consumption), control delays;\n",
    "\n",
    "Action types:\n",
    "\n",
    "| Action Type      | Example   |\n",
    "| ------------- | ------------- |\n",
    "| Continuous thrust | [ThrustÂ inÂ X,Â Y,Â Z] |\n",
    "| Discrete thrusters | ON/OFF for RCS jets |\n",
    "| Rotational control | \t[TorqueÂ inÂ Roll,Â Pitch,Â Yaw]|\n",
    "\n",
    "\n",
    "**Physics:**  \n",
    "Physics considerations and implementation plus formulas and general physics laws.  \n",
    "\n",
    "- Newtonian Dynamics:  \n",
    "For simpler free space and no gravity motion.  \n",
    "$ F = ma $ to update velocity and position (Newton's Second Law of Motion states that when a force acts on an object, it will cause the object to accelerate. The larger the object's mass, the greater the force will need to be to cause it to accelerate. This Law may be written as force = mass x acceleration).\n",
    "\n",
    "\n",
    "- Otbital Mechanics:  \n",
    "Consider orbital mechanics and forces from celestial bodies.  \n",
    "Every object in the universe attracts every other body in the universe with a force directed along the line of centers of the two objects\n",
    "that is proportional to the product of their masses and inversely proportional to the square of the distance between them,\n",
    "$$ F=\\frac{G_{m_1 m2}}{d^2} $$\n",
    "In this relation, $G$ is the Newton gravitational constant, $m_1$ and $m_2$ are the\n",
    "masses of the primary and secondary bodies, and $d$ is the distance between them.  \n",
    "Keplerâ€™s Equation: it relates the Mean Anomaly ($ð‘€$) to the Eccentric Anomaly ($ð¸$) for an orbit with eccentricity $ð‘’$:\n",
    "$$ M = E - e \\sin E $$\n",
    "where:\n",
    "    - $M$ = Mean Anomaly ($M = n(t - t_0)$), where $n = \\frac{2_\\pi}{T}$ is the mean motion, $T$ is the orbital period, and $t_0$ is a reference epoch.\n",
    "    - E = Eccentric Anomaly.\n",
    "    - e = Orbital eccentricity.  \n",
    "    Once $E$ is found, the True Anomaly $Î½$ (the angle between the position vector and the periapsis) can be determined using:\n",
    "$$ \\tan \\frac{v}{2} = \\sqrt{\\frac{1+e}{1-e}} \\tan \\frac{E}{2}$$\n",
    "The position in the orbital plane is given by:\n",
    "$$ r = \\frac{a(1 - e^2))}{1 + e \\cos v}$$\n",
    "where:\n",
    "    - $r$ = radial distance from the focus (central body).  \n",
    "    - $a$ = semi-major axis.  \n",
    "  The Cartesian coordinates in the orbital plane (before transformation to an inertial frame) are:\n",
    "$$ x = r \\cos v$$ $$ y = r \\sin v $$\n",
    "Implement with: https://www.astropy.org/ and https://docs.poliastro.space/en/stable/index.\n",
    "\n",
    "\n",
    "- Attitude Control:  \n",
    "Rotational dynamics for docking, planetery entry, or pointing. Usega of *quaternions* for rotation.\n",
    "Euler's theorem: The Orientation of a body is uniquely specified by a vector giving the direction of a body axis and a scalar specifying a rotation angle about the axis. \n",
    "\n",
    "- Fuel Consumption Model:  \n",
    "Thrust burns should reduce remaining fuel. Thruster inefficiencies should be considered. Usage of Tsiolkovskyâ€™s rocket equation:\n",
    "$$ \\varDelta v = v_e ln (\\frac{m_i0}{m_f}) $$\n",
    "where:\n",
    "    - $\\varDelta v$ is the change in velocity (delta-v);\n",
    "    - $ v_e $ is the exhaust velocity;\n",
    "    - $ m_0 $ is the initial mass of the rocket;\n",
    "    - $ m_f $ is the final mass of the rocket after the propellant is expended;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb03804-5b16-4d6e-8251-43bfa8eb0896",
   "metadata": {},
   "source": [
    "#### Reward Function\n",
    "Reward function should be shaped well and not only the end goal. The primary goal of the agent should be to reach the target, minimize fuel consumption, maintain stable motion, avoid oscillations, collisions and unsafe conditions. The reward function should balance all these goals.\n",
    "\n",
    "| Reward Type      | Description  |\n",
    "| ------------- | ------------- |\n",
    "| Goal proximity | Reward for getting closer to the target |\n",
    "| Fuel efficiency | Penalize excessive fuel use |\n",
    "| Soft landing | Bonus for smooth landings (low velocity) |\n",
    "| Docking precision | Bonus for aligning position & velocity |\n",
    "| Stability | Penalize excessive rotations |\n",
    "\n",
    "*Phase 1*: Reward based on moving towards the target.  \n",
    "*Phase 2*: Reward for slowing down near the target.  \n",
    "*Phase 3*: Reward only for successful docking.  \n",
    "\n",
    "A combiation of dense reward and spacrse reward can be applyed:\n",
    "- Dense rewards can speed up learning and will provide continuous feedback.\n",
    "- Sparse rewards will the given on task completion, but can be more challening for this scenario.\n",
    "\n",
    "**Reward function design:**\n",
    "1. Proximity: a proximity reward is provided to ensure that the spacecraft moves towards the goal. A **composite reward function** is used to combine multiple rewards.  \n",
    "*Composite rewards combine multiple reward signals into a single function. This is useful in complex tasks where multiple objectives need to be balanced.*  \n",
    "The distance based-reward will be calculated with the negative Euclidean distance:\n",
    "$$ r_{distance} = - \\lVert x - x_{target} \\rVert $$\n",
    "where $x$ is the current position and $x_{target}$ is the target position.\n",
    "> ```python\n",
    "> reward_distance = -np.linalg.norm(self.state[:3] - self.target_position)\n",
    "\n",
    "2. Efficiency: minimizes excessive thrust burns and can penalize thurs usage.  \n",
    "*Fuel penalty* where $a$ is the applied thrust vector:\n",
    "$$ r_{fuel} = - \\lVert a \\rVert $$\n",
    "> ```python\n",
    "> reward_fuel = -np.linalg.norm(action) * 0.01\n",
    "> \n",
    "For fuel *conservation*:  \n",
    "$$ r_{fuel} = \\frac{f_{remaining}}{f_{max}} $$\n",
    "> ```python\n",
    "> reward_fuel = self.state[-1] / self.max_fuel\n",
    ">\n",
    "3. Velocity: the agent needs to match the correct velocity and not overshoot. The velocity for precise docking has to be correct not to overshoot.\n",
    "$$ r_{velocity} = - \\lVert v - v{target} \\rVert $$\n",
    "> ```python\n",
    "> reward_velocity = -np.linalg.norm(self.state[3:6] - self.target_velocity)\n",
    ">\n",
    "4. Orientation: in order to dock a spacecraft needs to align correctly and penalizing misalignment:\n",
    "$$ r_{orientation} = -angle(q, q_{target}) $$\n",
    "where $q$ is the quaternion representing orientation.\n",
    "5. Landing: soft landing and encouraging stability. High velocity should be controlled during ladning anv penalized if the speed is to high:\n",
    "$$ r_{ladning} = -v^2_z $$\n",
    "6. Terminal rewards: in order to achiave correct behaviour, usage of large terminal rewards will be applyed for successfull missions. There will be a large penalty for failiures (for example crashes). Avoiding agent guessing and maximizing learning efficiency.\n",
    "\n",
    "A weighted sum of all the components of the reward function:\n",
    "$$ R = w1r_{distance} + w2r_{fuel} + w3r_{velocity} + w4r_{orientation} + w5r_{landing} + r_{goal} + r_{crash} $$\n",
    "\n",
    "```python\n",
    "reward = (\n",
    "    -np.linalg.norm(self.state[:3] - self.target_position) * 1.0  # Distance penalty\n",
    "    -np.linalg.norm(action) * 0.01  # Fuel efficiency penalty\n",
    "    -np.linalg.norm(self.state[3:6] - self.target_velocity) * 0.5  # Velocity matching\n",
    "    + orientation_penalty(self.state[6:10], self.target_orientation) * 0.3  # Orientation alignment\n",
    "    + (100 if docking_success else 0)  # Goal reward\n",
    "    + (-100 if collision_detected else 0)  # Collision penalty\n",
    ")\n",
    "```\n",
    "Avoid and consider:\n",
    "- avoid unintended ways to for the agent to maximize rewards;\n",
    "- sparce rewards can lead to less exploration of useful actions;\n",
    "- avoid over-penalizing fuel consumption since the agent might not move;\n",
    "- scalling reward terms in similar magnitude to achieve equall learning;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9da8c5-ab53-46ab-92df-de5c6ecfc1b4",
   "metadata": {},
   "source": [
    "#### Training and Evaluation\n",
    "Algorithms: Proximal Policy Optimization (PRO), Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3)  \n",
    "Hyperparameter tuning, learning rate, batch size, exploration strategies  \n",
    "\n",
    "Evaluation Metrics:\n",
    "1. Final Orbit Accuracy â€“ Does the spacecraft reach the target orbit within a small margin?\n",
    "2. Fuel Efficiency â€“ Does the agent minimize fuel usage?\n",
    "3. Number of Maneuvers â€“ Does the agent execute a minimal number of burns?\n",
    "4. Success Rate â€“ Percentage of successful orbital insertions across test episodes.\n",
    "5. Time Taken â€“ How many simulation steps are needed to reach the target?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b456-63b2-4832-a27a-591c8af745ca",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Inherit base `Gym` environment to define custom environment.  \n",
    "Set a base space environment and reuse at with wrappers for other environment setups like docking, orbit, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f0f062-088e-475b-b7a3-d2be62e3f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhech\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:180: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from gymnasium.envs import register\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.air import RunConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50cb984-c3b4-42e8-a5bd-1bffb4f84db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init navigation env\n",
    "class SpaceCraftNavigationEnv(gym.Env):\n",
    "    '''A basic custom spacecraft environment. Used as a base wrapper.'''\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "                \n",
    "        # Define observation space (position, velocity, fuel)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-1.0, -1.0, -1.0, -1.0, 0.0]),  # Min values (position, velocity, fuel)\n",
    "            high=np.array([1.0, 1.0, 1.0, 1.0, 1.0]),     # Max values\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define action space (thrust x, thrust y)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0, -1.0]), \n",
    "            high=np.array([1.0, 1.0]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.state = None  # Placeholder for spacecraft state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset state: Random position, zero velocity, full fuel\n",
    "        self.state = np.array([np.random.uniform(-1, 1), np.random.uniform(-1, 1), 0.0, 0.0, 1.0], dtype=np.float32)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y, vx, vy, fuel = self.state\n",
    "        thrust_x, thrust_y = action\n",
    "\n",
    "        # Update velocity based on action\n",
    "        vx += thrust_x * 0.01\n",
    "        vy += thrust_y * 0.01\n",
    "\n",
    "        # Update position\n",
    "        x += vx\n",
    "        y += vy\n",
    "\n",
    "        # Consume fuel\n",
    "        fuel = max(0, fuel - np.linalg.norm(action) * 0.01)\n",
    "\n",
    "        # Define reward (goal: reach [0,0] with fuel efficiency)\n",
    "        reward = -np.linalg.norm([x, y]) + fuel\n",
    "\n",
    "        # Check termination (reached goal or out of fuel)\n",
    "        done = (np.linalg.norm([x, y]) < 0.1) or (fuel <= 0)\n",
    "\n",
    "        self.state = np.array([x, y, vx, vy, fuel], dtype=np.float32)\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Position: {self.state[:2]}, Velocity: {self.state[2:4]}, Fuel: {self.state[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df68e6f-23e6-4c21-8d4d-1747fa5a8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhech\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment Spacecraft-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Register the environment with RLlib\n",
    "register_env(\"Spacecraft-v0\", lambda config: SpaceCraftNavigationEnv(config))\n",
    "register(\n",
    "    id=\"Spacecraft-v0\",\n",
    "    entry_point=\"__main__:SpaceCraftNavigationEnv\",\n",
    "    kwargs={\"config\": None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b28cc36-acbe-4cb8-a55b-de3a30b11464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-03 22:27:26</td></tr>\n",
       "<tr><td>Running for: </td><td>00:25:45.08        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.0/15.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">       num_env_steps_sample\n",
       "d_lifetime</th><th style=\"text-align: right;\">        ...env_steps_sampled\n",
       "_lifetime_throughput</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Spacecraft-v0_afa18_00000</td><td>TERMINATED</td><td>127.0.0.1:27584</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">          1531.1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">263.396</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 22:01:41,841\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-03 22:01:41,844\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-03 22:27:26,909\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/zhech/ray_results/PPO_2025-02-03_22-01-41' in 0.0158s.\n",
      "2025-02-03 22:27:27,192\tINFO tune.py:1041 -- Total run time: 1545.41 seconds (1545.06 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Train an agent with RLlib PPO\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "# Configure PPO training\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Spacecraft-v0\")\n",
    "    .env_runners(num_env_runners=2) # parallel rollouts\n",
    "    .training(\n",
    "        gamma=0.99, #discount factor ([0-1] closer to 1 makes the agent prioritize long-term rewards)\n",
    "        lr=1e-3, #learning rate\n",
    "        train_batch_size=4000\n",
    "    )\n",
    "    .framework(\"torch\") \n",
    ")\n",
    "\n",
    "# Train using Tune\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=RunConfig(stop={\"training_iteration\": 100}), # train for 100 iterations\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99bc92c-19aa-410f-a766-b58ec531db50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'timers': {'training_iteration': 15.355961166226495, 'restore_workers': 3.3750257361638816e-05, 'training_step': 15.355455823670509, 'env_runner_sampling_timer': 2.3035685787032194, 'learner_update_timer': 13.041798274708952, 'synch_weights': 0.008705616528840898, 'synch_env_connectors': 0.010918608491008928}, 'env_runners': {'timers': {'connectors': {'TensorToNumpy': np.float64(8.895721404496892e-05), 'RemoveSingleTsTimeRankFromBatch': np.float64(2.430166262614601e-06), 'AddObservationsFromEpisodesToBatch': np.float64(1.2009260273826356e-05), 'BatchIndividualItems': np.float64(2.8675487066060734e-05), 'AddStatesFromEpisodesToBatch': np.float64(1.0247948988452359e-05), 'NumpyToTensor': np.float64(4.7400056533005335e-05), 'UnBatchToIndividualItems': np.float64(3.178074787617437e-05), 'NormalizeAndClipActions': np.float64(8.404728050171263e-05), 'ListifyDataForVectorEnv': np.float64(4.738487275675073e-05), 'GetActions': np.float64(0.00032133654504573346)}}, 'episode_duration_sec_mean': 0.19676049699890427, 'weights_seq_no': 99.0, 'episode_return_max': 93.84134578704834, 'num_episodes_lifetime': 4201, 'agent_episode_returns_mean': {'default_agent': 59.92191906411201}, 'sample': np.float64(1.9862186303656573), 'episode_return_mean': 59.92191906411201, 'episode_len_mean': 152.7, 'num_module_steps_sampled': {'default_policy': 4000}, 'episode_len_min': 7, 'num_agent_steps_sampled': {'default_agent': 4000}, 'episode_return_min': -34.99599501490593, 'env_to_module_sum_episodes_length_in': np.float64(61.07192514554191), 'num_env_steps_sampled_lifetime': 400000, 'module_episode_returns_mean': {'default_policy': 59.92191906411201}, 'num_agent_steps_sampled_lifetime': {'default_agent': 400000}, 'env_to_module_sum_episodes_length_out': np.float64(61.07192514554191), 'num_module_steps_sampled_lifetime': {'default_policy': 400000}, 'num_env_steps_sampled': 4000, 'episode_len_max': 212, 'num_episodes': 26, 'time_between_sampling': np.float64(13.503210326342819), 'num_env_steps_sampled_lifetime_throughput': 263.3961851062948}, 'learners': {'__all_modules__': {'timers': {'connectors': {'AddObservationsFromEpisodesToBatch': 0.0010793163618705265, 'BatchIndividualItems': 0.2924721606618214, 'AddStatesFromEpisodesToBatch': 4.121834920765178e-05, 'AddOneTsToEpisodesAndTruncate': 0.012896706809935808, 'GeneralAdvantageEstimation': 0.04386505943335309, 'AddColumnsFromEpisodesToTrainBatch': 0.17430290796788503, 'NumpyToTensor': 0.2630120022688787}}, 'learner_connector_sum_episodes_length_out': 4041.501224183324, 'num_env_steps_trained': 4028, 'num_trainable_parameters': 135941.0, 'num_env_steps_trained_lifetime': 404400, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_module_steps_trained_lifetime': 404400, 'num_module_steps_trained': 4028, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained_lifetime_throughput': 265.2322492230067}, 'default_policy': {'vf_loss_unclipped': 22.804946899414062, 'curr_entropy_coeff': 0.0, 'weights_seq_no': 100.0, 'vf_explained_var': 0.8407624363899231, 'diff_num_grad_updates_vs_sampler_policy': 0.0, 'vf_loss': 1.0274266004562378, 'num_trainable_parameters': 135941.0, 'policy_loss': 0.0220208540558815, 'default_optimizer_learning_rate': 0.001, 'gradients_default_optimizer_global_norm': 4.793886184692383, 'total_loss': 1.0672608613967896, 'entropy': 0.49638259410858154, 'curr_kl_coeff': 1.7085938453674316, 'num_module_steps_trained': 4028, 'num_module_steps_trained_lifetime': 404400, 'num_non_trainable_parameters': 0.0, 'mean_kl_loss': 0.010425737127661705, 'module_train_batch_size_mean': 4041.501224183324}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 400000, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 263.3961851062948, 'perf': {'cpu_util_percent': np.float64(8.738095238095237), 'ram_util_percent': np.float64(70.25714285714285)}},\n",
       "  path='C:/Users/zhech/ray_results/PPO_2025-02-03_22-01-41/PPO_Spacecraft-v0_afa18_00000_0_2025-02-03_22-01-41',\n",
       "  filesystem='local',\n",
       "  checkpoint=Checkpoint(filesystem=local, path=C:/Users/zhech/ray_results/PPO_2025-02-03_22-01-41/PPO_Spacecraft-v0_afa18_00000_0_2025-02-03_22-01-41/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results\n",
    "results.get_best_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292a24e9-6172-4aa5-83a9-d86b1994196e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the trained policy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m algo \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mfrom_checkpoint(\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mget_best_result()\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test the trained agent\u001b[39;00m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m SpaceCraftNavigationEnv()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the trained policy\n",
    "algo = PPO.from_checkpoint(results.get_best_result().checkpoint.path)\n",
    "\n",
    "# Test the trained agent\n",
    "env = SpaceCraftNavigationEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # action = algo.compute_single_action(obs)\n",
    "    action = algo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db49d1-c503-46b8-b63e-ee7fce630794",
   "metadata": {},
   "source": [
    "Evaluation error BUG: https://github.com/ray-project/ray/issues/44475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b0c67-38fb-4f38-833f-86d45087bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point evaluation\n",
    "algo = PPO.from_checkpoint(\"C:/Users/zhech/ray_results/PPO_2025-02-02_19-34-57/PPO_Spacecraft-v0_05501_00000_0_2025-02-02_19-34-57/checkpoint_000000\")\n",
    "env = gym.make(\"Spacecraft-v0\")\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = algo.compute_single_action(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc132a46-b202-4b07-aafe-cf64b94ef58d",
   "metadata": {},
   "source": [
    "#### Experiments\n",
    "Perform multiple experiments and tests in separete notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc90ebe-b9dc-490c-9dd9-dc95831cf0b6",
   "metadata": {},
   "source": [
    "![Figure_approach](Figures/Approach_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25315246-afec-49ff-bb92-7481b9e20010",
   "metadata": {},
   "source": [
    "Agent = Algorithm: **PPO, SAC** for our tasks.\n",
    "How can these algorithms perform on our environments? Can the algorithm's hyperparameters be tuned to learn and grow it's rewards? Or should the environment be tuned, so that the algorithm is more effective?  \n",
    "How well can these algorithms perform on our environments? What do we measure?  \n",
    "\n",
    "How many time steps will each trial or run contain? If a terminal state is not reached after n steps, will we artificially terminate the episode? If the task is episodic, how many episodes should we run?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d889c4-d78c-4b1d-836f-6e3d92de0fab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (global)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
