{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77f0fa7-a0d7-4153-9900-b531d9de6aaa",
   "metadata": {},
   "source": [
    "## Outer Space with Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5dfb764-ead9-429b-b3c8-5b2ddf0a481d",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1200px;\">\n",
    "    \n",
    "#### Abstract\n",
    "Space exploration methods are being improved upon with every day. Humanity is on the forefront of embarking on the journy of exploring space and reach new points in development. Traversing the vast land of space is undertaken mostly by machines and crafts that are of course operated mainly by humans. It is very plausable that for the foreseen future the vast amount of exploration will be performed by machinery. Machines that we can easily call robots.  \n",
    "Robots were sent multiple times, most notably to Mars and perform very substansial exploration on the surface on their own. This is acompanied by humen supervision and instructions. Similarly in space enviroments, crafts perform multiple manuevers and desicions based on telemtry and computer programing.  \n",
    "Space crafts require mission control and multiple people on the ground to perform communication and frequent monitor. Artificial inteligence (AI) can step in and take control of space craft making them more effective and autonomouse. In general this evolution for robots and machinery seems natural. Due to the vast expanse of space and limitles possabilities in space exploration an AI robot can learn adapt to evnironments very effectively.  \n",
    "One process that produces good results is reinforecement learning (RL). In this situation the robot will be placed in an enivronment where it can perform actions. During traversing it will be rewarded or punished based on the action taken. This approach will be put to the test, leveraging multiple techniques. This work will present a starter point and explore multiple existing methods and technologies for RL.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5377062-5481-410e-8849-71e2fcc91915",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1200px;\">\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "##### Briefly on RL\n",
    "The main characters of RL are the agent and the environment. Agents are put in an unfamiliar environment and are rewarded or punished based on their actions. The main goal of the agent is to maximize the reward amount while traversing the environment. To describe is briefly, RL consists of **state** $s$ which is a complete description of the world. An **observation** $o$ is a description of the state which can be partial or full. Actions that an agent takes are mostly defined as **action space** which are the set of valid actions. Split into two, **discrete** where there is a finite number of moves available to the agent and **coninuous**, in which the agent controls a robot in a physical world. The type of action space will have an impact on what algorithm will be implemented. Importantly the agent, also performs much of it's reasoning via a policy. It is a rule used to decide what action to take. It can be deterministic ( $a_t = \\mu(s_t)$ ) or stochastic ( $a_t \\backsim \\pi(\\cdot \\mid s_t)$ ). Actions are formed from the policy, the sequence of actions is the trajectory which is the states and actions in the world. The goal is to maximize reward over a trajectory which would be defined by the **reward function** ($R$). It depends on the current action taken, the state of the world and the next state ($r_t = R (s_t, a_t, s_t + 1)$).  \n",
    "\n",
    "##### Words on environments\n",
    "Environment definition depends on the task at hand. While ther eare multiple avaiable environments for prototyping and experimentations, setting up a custom environment for this task was a primary goal. A custom environment however, requires substantial testing and debugging as well as some domain knowledge. A good approach is to start with a simple, base environment and run tests on top of it. Gradually increase the environment complexity and add further features.\n",
    "\n",
    "##### Approach\n",
    "The approach here is testing on multiple environments while mostly simple there are a few more complex aspects within them. The RL agents performed much better on simpler environments which were iteratevelly improved and updated. Multiple algorithms were tested and initial mesures and results are shown. Additionally, there are tests with popular frameworks and tools in the domain of RL. Some are more difficult to start with and some provide meaningfull details a bit better.  \n",
    "A description of how the reward scheme and policy is provided and further elaboration on the environment creation is also explained. The code implementation is available.  \n",
    "The methods and results are shared in the next section where general spacecraft mission challenges and ideas, example trajectories and implementations as well as results and comparisons.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0126b-8580-46bd-92b8-f37e6f6f4976",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"max-width: 1200px;\">\n",
    "\n",
    "#### Methods and Results\n",
    "There are a few questions that are a good start for these type of tasks: How will an algorithm perform on our environment? Can the algorithm's hyperparameters be tuned to learn and grow it's rewards? Or should the environment be tuned, so that the algorithm is more effective? What do we measure? How many time steps will each trial or run contain? If a terminal state is not reached after n steps, will we artificially terminate the episode? If the task is episodic, how many episodes should we run?\n",
    "\n",
    "Defining main parameters and structure:  \n",
    "- compute environment state into observation;\n",
    "- `reset()` function to initiate a new episode for the environment;\n",
    "- `step()` to compute the new state of the environment;\n",
    "\n",
    "\n",
    "When will a reward be given? \n",
    "- spacecraft's target destination;\n",
    "- efficiency or precision;\n",
    "\n",
    "When will termination occure?\n",
    "- goal reached;\n",
    "- loss of fuel;\n",
    "- collision with another object;\n",
    "- error in path resulting to the final destination being impossible to reach;\n",
    "\n",
    "Agent Goals:\n",
    "- Landing: Soft-landing on a planetary surface;\n",
    "- Interplanetary Transfer: Navigating from one celestial body to another;\n",
    "- Orbit Change: Transitioning between different orbits efficiently;\n",
    "\n",
    "**Observation space:**\n",
    "- perceiving the environment;\n",
    "- include enough details and information for the agent;\n",
    "- consider: framce of reference, coordinate system (Cartesian, Keplerian), sensor errors (noise);\n",
    "\n",
    "State variables:\n",
    "\n",
    "| Variable      | Description   |\n",
    "| ------------- | ------------- |\n",
    "| $ x, y, z $ | Position in space (Cartesian) or orbital elements |\n",
    "| $ v_x, v_y, v_z $ | Velocity vector |\n",
    "| $ \\theta, \\phi, \\psi $ | Orientation (Euler angles) |\n",
    "| $ w_x, w_y, w_z $ | Angular velocity |\n",
    "| $ f $ | Remaining fuel |\n",
    "| $ d_{target} $ | Distance to target |\n",
    "\n",
    "**Action space:**\n",
    "- thruster commands;\n",
    "- consider: thruster limitation, fuel efficiency (more thrust = more fuel consumption), control delays;\n",
    "\n",
    "Action types:\n",
    "\n",
    "| Action Type      | Example   |\n",
    "| ------------- | ------------- |\n",
    "| Continuous thrust | [Thrust¬†in¬†X,¬†Y,¬†Z] |\n",
    "| Discrete thrusters | ON/OFF for RCS jets |\n",
    "| Rotational control | \t[Torque¬†in¬†Roll,¬†Pitch,¬†Yaw]|\n",
    "\n",
    "\n",
    "**Physics:**  \n",
    "Physics considerations and implementation plus formulas and general physics laws.  \n",
    "\n",
    "- Newtonian Dynamics:  \n",
    "For simpler free space and no gravity motion.  \n",
    "$ F = ma $ to update velocity and position (Newton's Second Law of Motion states that when a force acts on an object, it will cause the object to accelerate. The larger the object's mass, the greater the force will need to be to cause it to accelerate. This Law may be written as force = mass x acceleration).\n",
    "\n",
    "\n",
    "- Otbital Mechanics:  \n",
    "Consider orbital mechanics and forces from celestial bodies.  \n",
    "Every object in the universe attracts every other body in the universe with a force directed along the line of centers of the two objects\n",
    "that is proportional to the product of their masses and inversely proportional to the square of the distance between them,\n",
    "$$ F=\\frac{G_{m_1 m2}}{d^2} $$\n",
    "In this relation, $G$ is the Newton gravitational constant, $m_1$ and $m_2$ are the\n",
    "masses of the primary and secondary bodies, and $d$ is the distance between them.  \n",
    "Kepler‚Äôs Equation: it relates the Mean Anomaly ($ùëÄ$) to the Eccentric Anomaly ($ùê∏$) for an orbit with eccentricity $ùëí$:\n",
    "$$ M = E - e \\sin E $$\n",
    "where:\n",
    "    - $M$ = Mean Anomaly ($M = n(t - t_0)$), where $n = \\frac{2_\\pi}{T}$ is the mean motion, $T$ is the orbital period, and $t_0$ is a reference epoch.\n",
    "    - E = Eccentric Anomaly.\n",
    "    - e = Orbital eccentricity.  \n",
    "    Once $E$ is found, the True Anomaly $ŒΩ$ (the angle between the position vector and the periapsis) can be determined using:\n",
    "$$ \\tan \\frac{v}{2} = \\sqrt{\\frac{1+e}{1-e}} \\tan \\frac{E}{2}$$\n",
    "The position in the orbital plane is given by:\n",
    "$$ r = \\frac{a(1 - e^2))}{1 + e \\cos v}$$\n",
    "where:\n",
    "    - $r$ = radial distance from the focus (central body).  \n",
    "    - $a$ = semi-major axis.  \n",
    "  The Cartesian coordinates in the orbital plane (before transformation to an inertial frame) are:\n",
    "$$ x = r \\cos v$$ $$ y = r \\sin v $$\n",
    "\n",
    "\n",
    "- Attitude Control:  \n",
    "Rotational dynamics for docking, planetery entry, or pointing. Usega of *quaternions* for rotation.\n",
    "Euler's theorem: The Orientation of a body is uniquely specified by a vector giving the direction of a body axis and a scalar specifying a rotation angle about the axis. \n",
    "\n",
    "- Fuel Consumption Model:  \n",
    "Thrust burns should reduce remaining fuel. Thruster inefficiencies should be considered. Usage of Tsiolkovsky‚Äôs rocket equation:\n",
    "$$ \\varDelta v = v_e ln (\\frac{m_i0}{m_f}) $$\n",
    "where:\n",
    "    - $\\varDelta v$ is the change in velocity (delta-v);\n",
    "    - $ v_e $ is the exhaust velocity;\n",
    "    - $ m_0 $ is the initial mass of the rocket;\n",
    "    - $ m_f $ is the final mass of the rocket after the propellant is expended;\n",
    "\n",
    "#### Reward Function\n",
    "Reward function should be shaped well and not only the end goal. The primary goal of the agent should be to reach the target, minimize fuel consumption, maintain stable motion, avoid oscillations, collisions and unsafe conditions. The reward function should balance all these goals.\n",
    "\n",
    "| Reward Type      | Description  |\n",
    "| ------------- | ------------- |\n",
    "| Goal proximity | Reward for getting closer to the target |\n",
    "| Fuel efficiency | Penalize excessive fuel use |\n",
    "| Soft landing | Bonus for smooth landings (low velocity) |\n",
    "| Docking precision | Bonus for aligning position & velocity |\n",
    "| Stability | Penalize excessive rotations |\n",
    "\n",
    "*Phase 1*: Reward based on moving towards the target.  \n",
    "*Phase 2*: Reward for slowing down near the target.  \n",
    "*Phase 3*: Reward only for successful docking.  \n",
    "\n",
    "A combiation of dense reward and spacrse reward can be applyed:\n",
    "- Dense rewards can speed up learning and will provide continuous feedback.\n",
    "- Sparse rewards will the given on task completion, but can be more challening for this scenario.\n",
    "\n",
    "**Reward function design:**\n",
    "1. Proximity: a proximity reward is provided to ensure that the spacecraft moves towards the goal. A **composite reward function** is used to combine multiple rewards.  \n",
    "*Composite rewards combine multiple reward signals into a single function. This is useful in complex tasks where multiple objectives need to be balanced.*  \n",
    "The distance based-reward will be calculated with the negative Euclidean distance:\n",
    "$$ r_{distance} = - \\lVert x - x_{target} \\rVert $$\n",
    "where $x$ is the current position and $x_{target}$ is the target position.\n",
    "> ```python\n",
    "> reward_distance = -np.linalg.norm(self.state[:3] - self.target_position)\n",
    "\n",
    "2. Efficiency: minimizes excessive thrust burns and can penalize thurs usage.  \n",
    "*Fuel penalty* where $a$ is the applied thrust vector:\n",
    "$$ r_{fuel} = - \\lVert a \\rVert $$\n",
    "> ```python\n",
    "> reward_fuel = -np.linalg.norm(action) * 0.01\n",
    "> \n",
    "For fuel *conservation*:  \n",
    "$$ r_{fuel} = \\frac{f_{remaining}}{f_{max}} $$\n",
    "> ```python\n",
    "> reward_fuel = self.state[-1] / self.max_fuel\n",
    ">\n",
    "3. Velocity: the agent needs to match the correct velocity and not overshoot. The velocity for precise docking has to be correct not to overshoot.\n",
    "$$ r_{velocity} = - \\lVert v - v{target} \\rVert $$\n",
    "> ```python\n",
    "> reward_velocity = -np.linalg.norm(self.state[3:6] - self.target_velocity)\n",
    ">\n",
    "4. Orientation: in order to dock a spacecraft needs to align correctly and penalizing misalignment:\n",
    "$$ r_{orientation} = -angle(q, q_{target}) $$\n",
    "where $q$ is the quaternion representing orientation.\n",
    "5. Landing: soft landing and encouraging stability. High velocity should be controlled during ladning anv penalized if the speed is to high:\n",
    "$$ r_{ladning} = -v^2_z $$\n",
    "6. Terminal rewards: in order to achiave correct behaviour, usage of large terminal rewards will be applyed for successfull missions. There will be a large penalty for failiures (for example crashes). Avoiding agent guessing and maximizing learning efficiency.\n",
    "\n",
    "A weighted sum of all the components of the reward function:\n",
    "$$ R = w1r_{distance} + w2r_{fuel} + w3r_{velocity} + w4r_{orientation} + w5r_{landing} + r_{goal} + r_{crash} $$\n",
    "\n",
    "```python\n",
    "reward = (\n",
    "    -np.linalg.norm(self.state[:3] - self.target_position) * 1.0  # Distance penalty\n",
    "    -np.linalg.norm(action) * 0.01  # Fuel efficiency penalty\n",
    "    -np.linalg.norm(self.state[3:6] - self.target_velocity) * 0.5  # Velocity matching\n",
    "    + orientation_penalty(self.state[6:10], self.target_orientation) * 0.3  # Orientation alignment\n",
    "    + (100 if docking_success else 0)  # Goal reward\n",
    "    + (-100 if collision_detected else 0)  # Collision penalty\n",
    ")\n",
    "```\n",
    "Avoid and consider:\n",
    "- avoid unintended ways to for the agent to maximize rewards;\n",
    "- sparce rewards can lead to less exploration of useful actions;\n",
    "- avoid over-penalizing fuel consumption since the agent might not move;\n",
    "- scalling reward terms in similar magnitude to achieve equall learning;\n",
    "\n",
    "\n",
    "#### Training and Evaluation\n",
    "Algorithms: Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3)  \n",
    "Hyperparameter tuning, learning rate, batch size, exploration strategies  \n",
    "\n",
    "Evaluation Metrics:\n",
    "1. Final Orbit Accuracy ‚Äì Does the spacecraft reach the target orbit within a small margin?\n",
    "2. Fuel Efficiency ‚Äì Does the agent minimize fuel usage?\n",
    "3. Number of Maneuvers ‚Äì Does the agent execute a minimal number of burns?\n",
    "4. Success Rate ‚Äì Percentage of successful orbital insertions across test episodes.\n",
    "5. Time Taken ‚Äì How many simulation steps are needed to reach the target?\n",
    "\n",
    "</div>\n",
    "\n",
    "##\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "\n",
    "#### Results and Implementation Details\n",
    "Starting out with a more traditional environment: the luner lander. It is available in Gymnaisum and can be improved upon with a custom DQN agent. Interacting with the hyperparameters is important to reach good results. After testing a few implementations which werent very good in terms of library execution and engine interaction, setting up a basic lunar lander with the help of Gymnaisum proved to be one of the first succefull atempts. Provided were the below hyperparameters to the custom DQN agent:\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "    <p>Running ~1300 episodes provided a good reward and the agent seemed to land succefully in the boundaries (figure 1). This can be improved and used a starting point.</p>\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "            <img src=\"Figures/Results/lunar_lander_plot.png\" alt=\"lunar\" style=\"max-width: 65%; max-height: 375px;\">\n",
    "\n",
    "<div style=\"width:10%\"></div>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99 \n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "UPDATE_EVERY = 4\n",
    "```\n",
    "</p>\n",
    "    </div>\n",
    "                <p style=\"margin-top: 5px; text-align: center; margin: 5px;\">figure 1.</p>\n",
    "</div>\n",
    "\n",
    "`insert details about custom DQN agent`\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "Using DQN to solve a similar problem with navigating to a specific target and tackling obstacles, however introduced was 3D space, in order to resemble a space environemnt. Starting off from simpler interactions and adding a few complexeties proved to be a good approach. Below are a set of hyperparameters used during experimentation:  \n",
    "</div>\n",
    "<div style=\"display: flex; justify-content: space-between; margin: 5px; max-width: 900px;\">\n",
    "\n",
    "<div style=\"width: 33%;\">\n",
    "        \n",
    ">**Initial testing:**\n",
    "   ```python\n",
    "       \"MlpPolicy\",\n",
    "       env,\n",
    "       learning_rate=1e-3,\n",
    "       buffer_size=10000,\n",
    "       learning_starts=500,\n",
    "       batch_size=64,\n",
    "       gamma=0.99,\n",
    "       target_update_interval=100,\n",
    "       train_freq=4,\n",
    "       verbose=1,\n",
    "   ```\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 33%;\">\n",
    "    \n",
    ">**From stable baseline docs:**\n",
    "```python\n",
    "    policy: 'MlpPolicy'\n",
    "    n_timesteps: !!float 1e5\n",
    "    learning_rate: !!float 6.3e-4\n",
    "    batch_size: 128\n",
    "    buffer_size: 50000\n",
    "    learning_starts: 0\n",
    "    gamma: 0.99\n",
    "    target_update_interval: 250\n",
    "    train_freq: 4\n",
    "    gradient_steps: -1\n",
    "    exploration_fraction: 0.12\n",
    "    exploration_final_eps: 0.1\n",
    "    policy_kwargs: net_arch=[256, 256]\n",
    "```\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 33%;\">\n",
    "        \n",
    ">**Improved results:**\n",
    "```python\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=5e-4,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=500,\n",
    "    batch_size=128,\n",
    "    gamma=0.95,\n",
    "    target_update_interval=100,\n",
    "    train_freq=10,\n",
    "    exploration_fraction=0.3,\n",
    "    verbose=1,\n",
    "```\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "In early training the results were good and the agent was very effective at reaching the goal and reaping the rewads unlike some other tests where agents requred much more steps to earn reward (figure 2). 500 000 training timesteps were used but termination is accomplished around 25 steps.  \n",
    "Observing the agent's path and manuavers can also be beneficil to improve since the agent should be able to take optimal moves and not explore mindlessly. This can be tweeked in the reward scheme but depends on the environment complexity.\n",
    "</div>\n",
    "<div style=\"display: flex; align-items: center; justify-content: space-between; max-width: 1200px; margin: 5px;\">\n",
    "    <div>\n",
    "        <img src=\"Figures/Results/navigate_dqn_01.png\" alt=\"nav\" style=\"width: 95%; max-height: 420px;\">\n",
    "        <p style=\"margin: 5px; text-align: center;\">figure 2</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"Figures/Results/navigate_dqn_02.png\" alt=\"nav\" style=\"width: 95%; max-height: 420px;\">\n",
    "        <p style=\"margin: 5px; text-align: center;\">figure 3</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Further experimentation was done with an improved environemnt and hyperparameters:  \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; max-width: 1200px; margin: 5px;\">\n",
    "    <div style=\"width: 48%;\">\n",
    "        \n",
    "**To improve the environment we can consider:**  \n",
    "Movement:\n",
    "- Introduce continuous movement using velocity and acceleration.\n",
    "- Apply Newton‚Äôs Second Law: $ F = ma $ where force from thrusters changes velocity.\n",
    "- Add momentum so movement persists unless countered.\n",
    "- Introduce gravity if near a celestial body.\n",
    "  \n",
    "Obstacles:\n",
    "- Irregular-shaped asteroids (bounding boxes or convex hulls).\n",
    "- Moving obstacles (asteroids or debris traveling with velocity).\n",
    "- Gravity wells (black holes or planetary gravity altering paths).\n",
    "  \n",
    "Anomalies:\n",
    "- Fuel leaks: Randomized decrease in fuel.\n",
    "- Sensor noise: Small noise added to state observations.\n",
    "- Thruster failures: Certain actions may not work sometimes.\n",
    "\n",
    "Sensors:\n",
    "- Limited field of view: The agent only knows nearby objects.\n",
    "- Lidar-style sensing: Returns distances to nearby objects.\n",
    "- Radar-based goal detection: The goal‚Äôs direction is known, but not distance.\n",
    "</div>\n",
    "<hr/>\n",
    "    <div style=\"width: 48%;\">\n",
    "\n",
    "**To improve the reward we can consider:**  \n",
    "- Reward efficient movement: Penalize unnecessary thrusts.\n",
    "- Encourage proximity to the goal: Small positive reward for getting closer.\n",
    "- Add time pressure: Slight penalty per step to encourage efficiency.\n",
    "\n",
    "- general spacecraft mission challenges and ideas\n",
    "- example missions, trajectories and implementations\n",
    "- technologies/ libraries used\n",
    "- configuring environments and experiments\n",
    "- detailed parameters and goals\n",
    "- every test performed\n",
    "- detailed algorithms\n",
    "- results and comparisons\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "\n",
    "This illustrated good results once again and the agent was reaching the goal seemingly in an optimal way. Now the reward seemed to be more gradual and spikes were eliminated which suggested a more linear model and refined refward scheme (figure 4).  \n",
    "![nav](Figures/Results/navigate_dqn_03.png)\n",
    "figure 4\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "Using the same space exploration environment which proved to be a good starting point and had potential for furhter configuration, a comparison was done between the algorithsm DQN, PPO and A2C. This test was important to understand how the agents differed and what hyperparameter changes and optimizations did to the results. The commulative reward reflects the total score achieved by the agent and gives a good indication of its overall success in the environment.\n",
    "After some tuning the agents started to see rewards and perform stabler overall. A2C was the one that had most fluctions and didn't provide good results in initial training. DQN and PPO on the other hand are much more stabler and are rising in terms of reward.\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: space-between; max-width: 1200px; margin: 5px;\">\n",
    "\n",
    "<div style=\"margin-right: 5px;\">\n",
    "\n",
    "![lunar](Figures/Results/comp_03.png)\n",
    "a\n",
    "\n",
    "</div>\n",
    "\n",
    "                           \n",
    "<div style=\"margin-right: 5px;\">\n",
    "\n",
    "![lunar](Figures/Results/comp_04.png)\n",
    "b\n",
    "\n",
    "</div>\n",
    "\n",
    "                           \n",
    "<div>\n",
    "\n",
    "![lunar](Figures/Results/comp_cm_04.png)\n",
    "c\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"max-width: 1200px; margin: 5px;\">\n",
    "DQN (Green) performs the best, achieving the highest and most stable rewards over time. PPO (Blue) is also stable and competitive but slightly lower than DQN in terms of rewards. Both DQN and PPO show steady improvement, stabilizing at higher rewards, while A2C remains volatile with generally negative performance. For the comulative results DQN accumulates rewards at a better rate than PPO, meaning it learns a more efficient policy over time.\n",
    "</div>\n",
    "\n",
    "figure 5\n",
    "\n",
    "Results for each algorithm: DQN, PPO, A2C, SAC (hyperparameters and tuning + plots)  \n",
    "Tools: Gym, Box2D\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda72d6a-85a7-4a69-ac98-99b4d700006a",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Complex environments require much more testing, visualisation and evaluating the results under the hood. To expand and learn from this project a very good start is to begin with predefined environments which are already fully setup and cofigured, improve them and check the results. From there onwards a simple environment can be setup as well as a simple policy and stable reward scheme and improve and update them over time. It is very usefull to take time and visualise all of the results and if possible visualise what the agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4987a5d-8d76-4380-beef-605db85306c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
